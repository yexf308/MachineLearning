{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/Module0/Intro_to_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMly5GMx1qMd"
      },
      "source": [
        "# Introduction to Machine learning\n",
        "\n",
        "## What is machine learning \n",
        "\n",
        "A popular defintion of **machine learning** or **ML**, due to Tom Mitchell:\n",
        "\n",
        "\"A computer program is said to learn from experience $E$ with respect to some class of tasks $T$, and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "One of my friends says: \n",
        "\n",
        "\"Machine learning is driving by looking at the rearview mirror.\"\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzFX3YZv8ui7"
      },
      "source": [
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/ml_map.png?raw=true\" width=\"900\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYoFc8os8_MM"
      },
      "source": [
        "## Main topics\n",
        "\n",
        "There are three basic machine learning paradigms: \n",
        "\n",
        "- supervised learning: Labeled data, direct feedback, predict future/outcome.\n",
        "\n",
        "- unsupervised learning: No labels, no feedback, find hidden struture in data.\n",
        "\n",
        "\n",
        "- reinforcement learning (not covered this course): Decision process, reward system.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO8oICbc-Ng8"
      },
      "source": [
        "Here are several examples of topics: \n",
        "\n",
        "1. Discrete supervised learning:     **classification**\n",
        "\n",
        "2. Continuous supervised learning:   **regression**\n",
        "\n",
        "3. Discrete unsupervised learning: **clustering** \n",
        "\n",
        "4. Continuous unsupervised learning: **dimensionality reduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaJtJhS-Dr_4"
      },
      "source": [
        "\n",
        "\n",
        "# Supervised learning \n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/01_02.png?raw=true\" width=\"600\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGP2M1zONbKV"
      },
      "source": [
        "## Setup of supervised learning\n",
        "Learn a function $F$, that maps an input to an output based on example input-output pairs \n",
        "\n",
        "$$\\{(x^{(i)}, y^{(i)}): i = 1, \\dots, n\\} $$\n",
        "\n",
        "with input $x^{(i)}$ and label $y^{(i)}$ such that \n",
        "\n",
        "$$ y^{(i)}=F(x^{(i)})+\\epsilon_i,  $$\n",
        "\n",
        "So for new input data $x$, we can make a prediction $y=F(x)$.\n",
        "\n",
        "\n",
        "## Parametric function approximation\n",
        "\n",
        "Parameterize the (abstract) mapping $F$  as a (concrete) function $F(x; \\theta):x \\mapsto y$  with  $y = F(x; \\theta)$\n",
        "\n",
        "-  $\\theta\\in\\mathbb{R}^d$: model parameters that determines the expression of $F$, and thus the performance of learning.\n",
        "\n",
        "-   **training**:  find the optimal $\\theta^\\ast$ using certain algorithms, such that on the training data $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^n$ , $F(x^{(i)}; \\theta^*)\\approx y^{(i)},  \\forall i \\quad \\mbox{in some sense} $\n",
        "\n",
        "- **prediction**:  for new data $x$, predict $y = F(x;\\theta^\\ast)$\n",
        "\n",
        "\n",
        "## Training\n",
        "\n",
        "Fit the model $F(x;\\theta)$ on training data $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\}_{i=1}^n$ \n",
        "\n",
        "- performance of rule $F(x;\\theta)$ on data sample $(x^{(i)},y^{(i)})$ is measured by a loss function $\\ell(F(x^{(i)}; \\theta),y^{(i)})$ with the property \n",
        "  $$\\ell(F, y) \\downarrow 0,  \\quad \\mbox{if } F \\to y\n",
        "  $$\n",
        "One possible lost function is $\\ell(F(x; \\theta), y) = (F(x; \\theta) - y)^2$\n",
        "\n",
        "\n",
        "- minimize the averaged loss function over $\\mathcal{D}$ w.r.t. $\\theta$:\n",
        " $$\n",
        " \\theta^* = \\arg\\min_{\\theta} \\; \\frac{1}{n} \\sum_{i=1}^n \\ell(F(x^{(i)}; \\theta),y^{(i)})\n",
        " $$\n",
        "\n",
        "- objective value = 0 means correct output for every training data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9ydDMBIS5Bx"
      },
      "source": [
        "## 1. Regression\n",
        "$y^{(i)}$ is a continuous variable. (Most covered in AMAT 565 (Applied Statistics for Data Scientist) and AMAT 593 (Practical)).\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/regression1.png?raw=true\" width=\"500\" />\n",
        "\n",
        "We can use polynormial to fit the datasets\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/regression2.png?raw=true\" width=\"500\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4RbE9E3RsV-"
      },
      "source": [
        "## 2. Classification\n",
        "\n",
        "$y^{(i)}$ is a discrete variable, for example, +- or integers. (Most covered in AMAT 592(Machine Learning) and partly covered in AMAT 565 (Applied Statistics for Data Scientists) and AMAT 593 (Practical) ). \n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/01_03.png?raw=true\" width=\"400\" />\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/digits.png?raw=true\" width=\"500\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ8tWUZwU_XP"
      },
      "source": [
        "## Testing and generalization\n",
        "\n",
        "**Testing/Validation**: examine the performance of $F(x;\\theta^*)$ on a new set of labeled data. (Most covered in AMAT 565 (Applied Statistics for Data Scientists), I will briefly mentioned in AMAT 592). \n",
        "\n",
        "- generalizability: the ability of trained model $F(x;\\theta^*)$ to react to new data. (Hold-out method)\n",
        " - divide the available data into two sets: training set and validation/testing set. \n",
        " - train on the training set and evaluate test error on validation set\n",
        " - adjust number of model parameters for best generalization on validation set\n",
        "\n",
        "- overfitting: trained too well but generalize poorly; usually caused by more parameters than can be justified by the data.\n",
        "\n",
        "- underfitting: bad fit on training & test sets\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/overfitting.png?raw=true\" width=\"600\" />\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/generalise.png?raw=true\" width=\"500\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrKTMY744Ui6"
      },
      "source": [
        "## More on supervised learning: non-paramatric learning\n",
        "\n",
        "- $k$-nearest neighbour\n",
        "\n",
        "- Deep learning (covered in AMAT 592(Machine Learning) and AMAT 593 (Practical) ) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX1fAAP66zlJ"
      },
      "source": [
        "## No free lunch theorem\n",
        "Unfortunately, there is no single best model that works optimally for all kinds of problems\n",
        "\n",
        "The best way is to pick a suitable model is based on domain knowledge, and/or trial and error \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UAizQ3Q5A-r"
      },
      "source": [
        "# Unsupervised learning\n",
        "\n",
        "Find  previously unknown patterns in data set  without pre-existing labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOCccZSN6ONJ"
      },
      "source": [
        "## 3. Clustering\n",
        "Group data in a way that data in the same group (i.e., cluster) are more similar (in some sense) to each other than to those in other clusters. (partly covered in AMAT 592 and mostly covered in AMAT 810 (Advanced Machine Learning) and AMAT 585 (Practical TDA))\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/01_06.png?raw=true\" width=\"500\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urrIL2OPBPW2"
      },
      "source": [
        "## 4. Dimensional Reduction\n",
        "\n",
        "Dimensionality reduction technique that emphasizes variation in a dataset.  (partly covered in AMAT 592 and mostly covered in AMAT 810 (Advanced Machine Learning) )\n",
        "\n",
        "Here in the example, Direction of largest\n",
        "variation is relevant. The rest is “noise”. This is the method of **Principle component analysis**. \n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/pca_demo.png?raw=true\" width=\"500\" />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3TAnSnfOCRl"
      },
      "source": [
        "## Dimension reduction for data compression\n",
        "\n",
        "Manifold learning\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/01_07.png?raw=true\" width=\"600\" />\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPVfYgRjRognNfukcmp/uZy",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Intro_to_ML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
