{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "19.GMM3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN2aqYhu9w5DVORyAkTLHZj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yexf308/MAT592/blob/main/19_GMM3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5rLmCqsYT5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1f3e6e-4545-4191-dea2-a83c9bf81c8a"
      },
      "source": [
        "%pylab inline \n",
        "from IPython.display import Image\n",
        "import numpy.linalg as LA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmNg_brTY8HR"
      },
      "source": [
        "#Gaussian Mixture Models: Variations\n",
        "##  Hidden Markov Models and Gaussian Mixture Models\n",
        "\n",
        "- In hidden Markov models, we use \"hidden state\" instead of \"clusters\". Still we assume there are $K$ hidden states. \n",
        "\n",
        "- Some background of Markov chain is needed here. \n",
        " -  Markovian property \n",
        " - Property of the transition matrix $M_{ij}$ \n",
        " - Invariant distribution of Markov chain. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "GxbG1Exe9Rwy",
        "outputId": "7694aa06-a308-4696-8d93-1c62c3d42dc6"
      },
      "source": [
        "display(Image(url='https://github.com/yexf308/MAT592/blob/main/image/HMM.png?raw=true', width=500))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/yexf308/MAT592/blob/main/image/HMM.png?raw=true\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jDZYd1daDdc"
      },
      "source": [
        "Hidden Markov Models assumes that observed dataset $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N\\subset \\mathbb{R}^d$ are sampled through a generative process:\n",
        "\n",
        "- Generate the sequence of states $\\{z^{(t)}\\}_{t=1}^N$ by a Markov chain whose transition matrix is $M\\in \\mathbb{R}^{K\\times K}$ starting the initial distribution as the invariant distribution $\\vec\\pi$.\n",
        "\n",
        " **Assumption 1:** We assume the Markov chain has reached the stationary,  \n",
        " $$P(z^{(1)})=\\vec\\pi, \\qquad \\vec\\pi = M\\vec\\pi$$\n",
        "\n",
        " **Assumption 2:** the state at time $t$, $z^{(t)}$, depends only on the state at the previous time $t-1$, $z^{(t-1)}$, i.e.,\n",
        " $$ P(z^{(t)}|z^{(1:t-1)})=P(z^{(t)}|z^{(t-1)})$$\n",
        "\n",
        "\n",
        "- Given the state $z^{(t)}=c$, generate a point $\\mathbf{x}^{(t)}$ from the associated multivariate Gaussian distribution $\\mathcal{N}(\\mu_c, \\Sigma_c)$ with PDF. It is also called **emission distribution**:\n",
        "$$p(\\mathbf{x}|\\mu_c, \\Sigma_c)= \\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma_c|}}\\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\mu_c)^\\top\\Sigma_c^{-1}(\\mathbf{x}-\\mu_c)\\right)$$\n",
        " **Assumption 3**: The observed data at time $t$, $\\mathbf{x}^{(t)}$ depends only on the state at time $t$, $z^{(t)}$, i.e.,\n",
        " $$P(\\mathbf{x}^{(t)}|z^{(1:t)}) =P(\\mathbf{x}^{(t)}|z^{(t)})$$ \n",
        "\n",
        "\n",
        "Now the parameter $\\theta=\\{\\pi_c, \\mu_c, \\Sigma_c,  M_{ij}\\}_{c,i,j=1}^K$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv2p-cYMqdEt"
      },
      "source": [
        "## Likelihood of HMM\n",
        "\n",
        "### Forward probability\n",
        "\n",
        "The likelihood $\\ell(\\theta)=P(\\mathbf{x}^{(1:N)}|\\theta)$. Similarly,\n",
        "$$P(\\mathbf{x}^{(1:t)}|\\theta)=\\sum_{c=1}^K \\underbrace{P(\\mathbf{x}^{(1:t)}, z^{(t)}=c|\\theta)}_{\\triangleq\\alpha_t(c)} $$\n",
        "\n",
        "However, \n",
        "$$P(\\mathbf{x}^{(1:N)}|\\theta)\\ne \\Pi_{i=1}^N \\sum_{c=1}^K \\left(p(\\mathbf{x}^{(i)}, z^{(i)}=c|\\theta) \\right)$$\n",
        "Because these hidden state are not sampled independently!\n",
        "\n",
        "$\\alpha_t(c)$, is also called **forward probability**. \n",
        "\n",
        "\\begin{align}\n",
        "\\alpha_t(c) &= P(\\mathbf{x}^{(1:t)}, z^{(t)}=c|\\theta) = \\sum_{i=1}^K P(\\mathbf{x}^{(1:t)}, z^{(t)}=c, z^{(t-1)}=i|\\theta) \\\\\n",
        "&=\\sum_{i=1}^K P(\\mathbf{x}^{(t)}| z^{(t)}=c, z^{(t-1)}=i, \\mathbf{x}^{(1:t-1)}, \\theta)\\cdot P(z^{(t)}=c |z^{(t-1)}=i, \\mathbf{x}^{(1:t-1)}, \\theta)\\cdot P(\\mathbf{x}^{(1:t-1)}, z^{(t-1)}=i|\\theta) \\\\\n",
        "&= \\sum_{i=1}^K P(\\mathbf{x}^{(t)} |\\mu_c, \\Sigma_c) M_{ic}\\alpha_{t-1}(i)\n",
        "\\end{align}\n",
        "\n",
        "If we use linear algebra, the recurrsion formular has the following compact form, define $\\vec\\alpha_t = [\\alpha_t(1), \\dots, \\alpha_t(K)]$, $B(\\mathbf{x}^{(t)}) =\\text{diag}([P(\\mathbf{x}^{(t)} |\\mu_1, \\Sigma_1), \\dots, P(\\mathbf{x}^{(t)} |\\mu_K, \\Sigma_K)]) $, then\n",
        "$$\\vec\\alpha_t =\\vec\\alpha_{t-1} M B(\\mathbf{x}^{(t)}) $$\n",
        "For convenience, set $\\vec\\alpha_0=\\vec\\pi$. So the forward probability and likelihood are\n",
        "$$\\vec\\alpha_N= \\vec \\pi M B(\\mathbf{x}^{(1)})M B(\\mathbf{x}^{(2)})\\dots M B(\\mathbf{x}^{(N)})$$\n",
        "\n",
        "$$ \\ell(\\theta)= \\vec\\alpha_N \\mathbb{1}^\\top$$\n",
        "where $\\mathbb{1}$ is a row of ones with length $K$. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5vRwF5CWD0L"
      },
      "source": [
        "### Backward probability \n",
        "Define the **backward probability**, $\\beta_t(c) = P(\\mathbf{x}^{(t+1:N)}|z^{(t)}=c, \\theta)$. Note \n",
        "\\begin{align}\n",
        "P(\\mathbf{x}^{(t+1:N)}|z^{(t)}=c, \\theta) &= \\sum_{i=1}^K P(\\mathbf{x}^{(t+1:N)}|z^{(t+1)}=i, \\theta)P(z^{(t+1)}=i|z^{(t)}=c, \\theta) \\\\\n",
        "&=\\sum_{i=1}^K P(\\mathbf{x}^{(t+1)}|z^{(t+1)}=i, \\theta)P(z^{(t+1)}=i|z^{(t)}=c, \\theta)P(\\mathbf{x}^{(t+2:N)}|z^{(t+1)}=i, \\theta)\n",
        "\\end{align}\n",
        "Then \n",
        "\\begin{align}\n",
        "\\beta_t(c)  = \\sum_{i=1}^K P(\\mathbf{x}^{(t+1)}|\\mu_i, \\Sigma_i)M_{ci} \\beta_{t+1}(i)\n",
        "\\end{align}\n",
        "Define $\\vec\\beta_t =[\\beta_t(1),\\dots, \\beta_t(K)]^T$.\n",
        "\n",
        "For convenience, set $\\vec\\beta_T=[1, \\dots, 1]^T$, the recursion formula has the following compact form.\n",
        "\n",
        "$$\\vec\\beta_{t}=MB(\\mathbf{x}^{(t+1)})\\vec\\beta_{t+1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3MReKKxVRMF"
      },
      "source": [
        "### Log-sum-exp trick\n",
        "In practice, naively implementing forward algorithm will never work due to arithmetic underflow or overflow. \n",
        "\n",
        "Define $g_t(c)=\\log \\alpha_t(c)$, then \n",
        "\\begin{align}\n",
        " g_t(c) = \\log \\alpha_t(c) = \\log \\sum_{i=1}^K \\exp\\left(l(z^{(t)}=c|z^{(t-1)}=i) + l(\\mathbf{x}^{(t)}|z^{(t)}=c)+g_{t-1}(i)\\right)\n",
        "\\end{align}\n",
        "denoting $l=\\log P$\n",
        "\n",
        "To calculate this, one has to use **log-sum-exp** trick:\n",
        "\n",
        "let’s suppose we would like to compute $\\log\\sum_{i=1}^m\\exp(a_i)$, \n",
        "\n",
        "- choose $b=\\max_{i}a_i$.\n",
        "\n",
        "- \\begin{align}\n",
        "\\log\\sum_{i=1}^m\\exp(a_i) =\\log\\sum_{i=1}^m\\exp(a_i-b)\\exp(b)  = b+\\log\\sum_{i=1}^m\\exp(a_i-b)\n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyKRuXeKM_s8"
      },
      "source": [
        "## Expectation-Maximization(EM) algorithm: the general version\n",
        "\n",
        "### The general version\n",
        "The goal of EM is to find a maximum likelihood estimate (MLE) estimate in models involving hidden/latent/unobserved variables/data.\n",
        "\n",
        "- Observed data $\\mathbf{x}^{(1:N)}$.\n",
        "\n",
        "- Model $(X, Z)\\sim P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$. Here $z^{(1:N)}$ represents some collection of unobserved variables. Note EM works\n",
        "best when $P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$ is an exponential family. \n",
        "\n",
        "- **Goal:** Find $\\theta^*=\\arg\\max_{\\theta} P(\\mathbf{x}^{(1:N)}|\\theta)$. Note the likelihood $P(\\mathbf{x}^{(1:N)}|\\theta) = \\sum_{z^{(1:N)}}P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$. \n",
        "\n",
        "- **EM Algorithm:**\n",
        " - Initialize $\\theta^1$. \n",
        "\n",
        " - For $t=1, 2, \\dots$ until some convergence criterion is met,\n",
        "\n",
        "    - E-step: Compute the  auxiliary function\n",
        "\n",
        "    \\begin{align} \\boxed{Q(\\color{blue}\\theta, \\color{red}{\\theta^t})=\\mathbb{E}_{Z|X, \\color{red}{\\theta^t}}\\left[\\log P(X, Z|X=\\mathbf{x}^{(1:N)} ,\\color{blue}\\theta)\\right]   = \\sum_{z^{(1:N)}} P(z^{(1:N)} |\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})\\log P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\color{blue} \\theta)}\n",
        "    \\end{align}\n",
        "\n",
        "    - M step: Solve for $\\color{red}{\\theta_{t+1}} = \\arg\\max_\\theta Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$\n",
        "\n",
        "- In practice, we will be able to represent $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$  analytically as a function $\\theta$. Moreover, we will\n",
        "often be able to analytically maximize $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$. \n",
        "\n",
        "- It is usually a good idea to introduce some randomization into the initialization $\\theta^1$. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lswMOgiWdi2J"
      },
      "source": [
        "### Proof of correctness\n",
        "\n",
        "Expectation-maximization works to improve $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$ rather than directly improving the log likelihood $\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta)$. Here it is shown that improvements to the former imply improvements to the latter.\n",
        "\n",
        "By Baye's rule, for any $z^{(1:N)}$ with non-zero probability  $P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{blue}\\theta)$, \n",
        "\n",
        "$$\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta) = \\log P(\\mathbf{x}^{(1:N)}, z^{(1:N)}|\\color{blue}\\theta) - \\log P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{blue}\\theta) $$\n",
        "\n",
        "By multiplying both sides $P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{red}{\\theta^t})$ and summing over $z^{(1:N)}$, (which is equvilently, take the expectation over possible values of the hidden variable $Z$ under the current parameter estimate $\\color{red}{\\theta^t}$)\n",
        "\\begin{align}\n",
        "\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta)&=\\sum_{z^{(1:N)}}P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{red}{\\theta^t})\\log P(\\mathbf{x}^{(1:N)}, z^{(1:N)}|\\color{blue}\\theta) -\\sum_{z^{(1:N)}} P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{red}{\\theta^t})\\log P(z^{(1:N)}|\\mathbf{x}^{(1:N)}, \\color{blue}\\theta) \\\\\n",
        "& =Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) +H(\\color{blue}\\theta|\\color{red}{\\theta^t})\n",
        "\\end{align}\n",
        "\n",
        "when $\\theta= \\theta^t$, \n",
        "$$\\log P(\\mathbf{x}^{(1:N)}|\\color{red}{\\theta^t})=Q(\\color{red}{\\theta^{t}}, \\color{red}{\\theta^t}) +H(\\color{red}{\\theta^t}|\\color{red}{\\theta^t}) $$\n",
        "\n",
        " Subtracting this last equation from the previous equation gives\n",
        "\\begin{align}\n",
        " \\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta) -\\log P(\\mathbf{x}^{(1:N)}|\\color{red}{\\theta^t}) &=  Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) - Q(\\color{red}{\\theta^{t}}, \\color{red}{\\theta^t}) +H(\\color{blue}\\theta|\\color{red}{\\theta^t})-H(\\color{red}{\\theta^t}|\\color{red}{\\theta^t})   \n",
        "  \\end{align}\n",
        "  \\begin{align}\n",
        " \\boxed{\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta) -\\log P(\\mathbf{x}^{(1:N)}|\\color{red}{\\theta^t}) \\ge Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) - Q(\\color{red}{\\theta^{t}}, \\color{red}{\\theta^t})}\n",
        " \\end{align}\n",
        "\n",
        " It is due to the Gibbs' inequality that $H(\\color{blue}\\theta|\\color{red}{\\theta^t})\\ge H(\\color{red}{\\theta^t}|\\color{red}{\\theta^t})$.\n",
        "\n",
        " So choose $\\theta$ to maximize $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$ causes $\\log P(\\mathbf{x}^{(1:N)}|\\color{blue}\\theta)$ improves at least that much. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcOB_fttNS0O"
      },
      "source": [
        "## EM for HMM: Baum–Welch algorithm\n",
        "\n",
        "### E-step\n",
        "The joint probability $P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)$ is \n",
        "\\begin{align}\n",
        "P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta)&=P(z^{(1)})\\cdot\\Pi_{i=1}^{N-1}P(z^{(i+1)}|z^{(i)},\\theta) \\cdot\\Pi_{j=1}^N P(\\mathbf{x}^{(j)}|z^{(j)},\\theta) \\\\\n",
        "&=\\pi_{z^{(1)}}\\cdot\\Pi_{i=1}^{N-1} M_{z^{(i)}z^{(i+1)}} \\cdot \\Pi_{j=1}^N P(\\mathbf{x}^{(j)}|\\mu_{z^{(j)}},\\Sigma_{z^{(j)}})\n",
        "\\end{align}\n",
        "Then \n",
        "\\begin{align}\n",
        "\\log P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\theta) = \\log \\pi_{z^{(1)}} + \\sum_{i=1}^{N-1} \\log M_{z^{(i)}z^{(i+1)}} + \\sum_{j=1}^N \\log P(\\mathbf{x}^{(j)}|\\mu_{z^{(j)}},\\Sigma_{z^{(j)}})\n",
        "\\end{align}\n",
        "\n",
        "The  auxiliary function $Q(\\color{blue}\\theta, \\color{red}{\\theta^t})$ is,\n",
        "\n",
        "\\begin{align}\n",
        "Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) &= \\sum_{z^{(1:N)}} P(z^{(1:N)} |\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t}) \\log P(\\mathbf{x}^{(1:N)} ,z^{(1:N)} |\\color{blue}\\theta)  \\\\\n",
        "&= \\sum_{z^{(1)}}\\boxed{P(z^{(1)}|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})}\\log \\pi_{z^{(1)}}+\\sum_{i=1}^{N-1}\\sum_{z^{(i)}z^{(i+1)}} \\boxed{P(z^{(i:i+1)}|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t}) }\\log M_{z^{(i)}z^{(i+1)}} + \\sum_{j=1}^N \\sum_{z^{(j)}}\\boxed{P(z^{(j)}|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})}\\log P(\\mathbf{x}^{(j)}|\\mu_{z^{(j)}},\\Sigma_{z^{(j)}}) \\\\\n",
        "& =  \\sum_{k=1}^K\\color{red}{\\gamma_{k}^t(1)}\\log \\color{blue}\\pi_{k}+\\sum_{i=1}^{N-1}\\sum_{k,c=1}^K \\color{red}{\\xi_{kc}^t(i)}\\log\\color{blue}{M_{kc}} + \\sum_{j=1}^N \\sum_{k=1}^K \\color{red}{\\gamma_{k}^t(j)}\\log P(\\mathbf{x}^{(j)}|\\color{blue} {\\mu_{k},\\Sigma_{k}})\n",
        "\\end{align}\n",
        "\n",
        "where $\\color{red}{\\gamma_{k}^t(i)} = P(z^{(i)}=k|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t})$ and $\\color{red}{\\xi_{kc}^t(i)}= P(z^{(i)}=k, z^{(i+1)}=c|\\mathbf{x}^{(1:N)} ,\\color{red}{\\theta^t}) $. How to find $\\gamma_{k}(i)$ and $\\xi_{kc}(i)$?\n",
        "\n",
        "\n",
        "\n",
        "### M-step \n",
        " For the M-step, we need to find a value of $\\theta$ maximizing $Q(\\theta, \\theta^t)$.  Fortunately, it turns out that we can often do this analytically. \n",
        "\n",
        " - First, to maximize with respect to $\\mu_k$ and $\\Sigma_k$, i.e., \n",
        " $$ 0= \\nabla_{\\mu_k}Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) = \\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\nabla_{\\mu_k} \\log P(\\mathbf{x}^{(j)}|\\color{blue}{\\mu_{k},\\Sigma_{k}}) $$\n",
        "\n",
        "  $$ 0= \\nabla_{\\Sigma_k}Q(\\color{blue}\\theta, \\color{red}{\\theta^t}) = \\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\nabla_{\\Sigma_k} \\log P(\\mathbf{x}^{(j)}|\\color{blue}{\\mu_{k},\\Sigma_{k}}) $$\n",
        "\n",
        "  Then we have the analytical solution which is the solution of weighted MLE\n",
        "  $$\\mu_k^{t+1} = \\frac{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\mathbf{x}^{(j)}}{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)}} $$\n",
        "\n",
        "$$\\Sigma_k^{t+1} =\\frac{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)} \\left[\\mathbf{x}^{(i)}-\\mu_k^{t+1}\\right]\\left[\\mathbf{x}^{(i)}-\\mu_k^{t+1}\\right]^T }{\\sum_{j=1}^N \\color{red}{\\gamma_{k}^t(j)}} $$\n",
        "\n",
        "- Second, to maximize with respect to $\\vec\\pi$. The constraint is $\\sum_{k}\\pi_k = 1$. We can do this analytically using the method of Lagrange multipliers, the result is \n",
        " \n",
        " $$ \\pi_k^{t+1} =\\frac{\\gamma_k^t(1)}{\\sum_{c=1}^K \\gamma_c^t(1)} $$\n",
        "\n",
        "\n",
        "- Third, to maximize with respect to the Markov transition matrix $M$. The contraint is $\\sum_{c=1}^K M_{kc} =1$. We can do this analytically using\n",
        "Lagrange multipliers as well. \n",
        "\n",
        "$$M_{kc}^{t+1}= \\frac{\\sum_{j=2}^N \\xi_{kc}^t(j)}{\\sum_{j=2}^N \\sum_{c=1}^K \\xi_{kc}^t(j)}. $$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2W9jRtK68dB"
      },
      "source": [
        "### To find $\\gamma_{k}(i)$ and $\\xi_{kc}(i)$: Forward-Backward Algorithm \n",
        "\\begin{align}\n",
        "\\gamma_{k}(i) & = P(z^{(i)}=k|\\mathbf{x}^{(1:N)}, \\theta)\\\\\n",
        "&=\\frac{P(z^{(i)}=k, \\mathbf{x}^{(1:N)}|\\theta)}{P(\\mathbf{x}^{(1:N)}|\\theta)}\\\\\n",
        "&=\\frac{P(z^{(i)}=k, \\mathbf{x}^{(1:i)}|\\theta)P(\\mathbf{x}^{(i+1:N)}|z^{(i)}=k, \\theta)}{P(\\mathbf{x}^{(1:N)}|\\theta)} \\\\\n",
        "&= \\frac{\\alpha_i(k)\\beta_i(k)}{P(\\mathbf{x}^{(1:N)}|\\theta)}\n",
        "\\end{align}\n",
        "This is because in the hidden markov model, \n",
        "$$ P(\\mathbf{x}^{(i+1:N)}|z^{(i)}=k, \\mathbf{x}^{(1:i)})= P(\\mathbf{x}^{(i+1:N)}|z^{(i)}=k).$$\n",
        "\n",
        "If we look at the MLE, $\\gamma_k(i)$ shows up in both denorminator and numerator. So what matters is the relative ratio $\\gamma_k(i)$ for different $i$. \n",
        "\n",
        "So we can renormalized $\\vec\\alpha_i$ and $\\vec\\beta_i$ at each $i$. \n",
        "\n",
        "Similarly, \n",
        "\\begin{align}\n",
        "\\xi_{kc}(i)&= P(z^{(i)}=k, z^{(i+1)}=c|\\mathbf{x}^{(1:N)} ,\\theta) \\\\\n",
        "&=\\frac{P(z^{(i)}=k, z^{(i+1)}=c,\\mathbf{x}^{(1:N)}|\\theta )}{P(\\mathbf{x}^{(1:N)}|\\theta )} \\\\\n",
        "& = \\frac{P(z^{(i)}=k, \\mathbf{x}^{(1:i)}|\\theta)P(\\mathbf{x}^{(i+2:N)}|z^{(i+1)}=c, \\theta)P(z^{(i+1)}=c|z^{(i)}=k)P(\\mathbf{x}^{(i+1)}|z^{(i+1)}=c)}{P(\\mathbf{x}^{(1:N)}|\\theta )} \\\\\n",
        "&=\\frac{\\alpha_i(k)\\beta_{i+1}(c)M_{kc}P(\\mathbf{x}^{(i+1)}|\\mu_c, \\Sigma_c)}{P(\\mathbf{x}^{(1:N)}|\\theta)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}